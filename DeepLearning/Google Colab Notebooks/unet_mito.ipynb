{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "unet_mito_pytorch_best.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_uZJQKKwKd9",
        "colab_type": "text"
      },
      "source": [
        "**This Google Colab notebook implements a U-Net designed by the Allen Institute** (https://github.com/AllenCellModeling/pytorch_fnet)\n",
        "\n",
        "Typically, training a model requires access to GPUs. Google provides free GPUs via their platform Google Colaboratory so I'll be using that for training. One can then save the model parameters and run model prediction locally - this step shouldn't require a GPU.**\n",
        "\n",
        "Google Colab is a Jupyter Notebook style platform. To enable GPU, click on \"Runtime --> Change runtime type\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEA8DP051aXd",
        "colab_type": "text"
      },
      "source": [
        "**<font color='green'>Parameters</font>**\n",
        "\n",
        "\n",
        "\n",
        "*IMG_WIDTH*, *IMG_HEIGHT*\n",
        "  \n",
        "  - image dimensions; you can choose dimensions =/ your input and the code will reshape to new dimensions</font>\n",
        "\n",
        "*batch_size*\n",
        " \n",
        "\n",
        "  - optimization is done in batches to avoid violating memory constraints of hardware. Choose a *batch_size* that will give representative subsets of the data - optimization is ultimately an average over gradients computed on each batch, so non-representative batches will mislead the gradient descent and lead to non-optimal results (ie, decrease our likelihood of reaching true and reasonable minima of the loss function)\n",
        "\n",
        "*learning_rate*\n",
        "\n",
        "  - determines the stepsize of the gradient descent. Typical range is 0.0001 - 1, from slow - fast learning. *learning_rate* is usually heuristically determined - there is no universal formula\n",
        "\n",
        "*numclasses*\n",
        "\n",
        "  - This model is designed for 2 classes: foreground and background. So *numclasses*=2\n",
        "\n",
        "*mult_chan*\n",
        "\n",
        "  - Vestige of AllenInstitute implementation. This model only works with 1 channel of image data so *mult_chan* = 1\n",
        "\n",
        "*epochs*\n",
        "\n",
        "  - Training duration. In general, keep learning until you either can't or won't! No universal formula for this either\n",
        "\n",
        "*betas*\n",
        "\n",
        "  - Coefficients to do fancy stuff during Adam optimization. We can generally keep these default. \n",
        "\n",
        "*depth*\n",
        "\n",
        "  - How many layers deep do you want U-Net? Typically 4-6. *My current implementation fails after something like depth=8, which I have to fix. Another vestige from AllenInstitute implementation* \n",
        "\n",
        "*contrast/brightness/saturation*\n",
        "\n",
        "  - Values for data augmentation. E.g., *contrast = 0.1* will randomly apply some contrast between 0 and 10% to our training images. This helps us prevent overfitting (kind of like the transational invariance but more abstract)\n",
        "\n",
        "*optimizer_fx*\n",
        "\n",
        "  - Choose your optimization scheme. \n",
        "    - *Adam*\n",
        "    - *SGD*\n",
        "\n",
        "*loss_fx*\n",
        "\n",
        "  - Choose your loss function. \n",
        "    - *BCEWithLogitsLoss*\n",
        "    - *BCELoss*\n",
        "    - *CrossEntropyLoss*\n",
        "    - *NLLLoss*\n",
        "    - *MLELoss* "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRymn96hDDA3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#  First, mount Google Drive. This let's us store data on Google Drive \n",
        "#  and access it (relatively quickly) from inside this notebook\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPlIJ6bQpuiX",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title libraries\n",
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import torch\n",
        "from torchsummary import summary\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "import os\n",
        "import numpy as np\n",
        "from skimage.io import imread\n",
        "from skimage.transform import resize\n",
        "from IPython.display import clear_output\n",
        "from albumentations import (HorizontalFlip, ShiftScaleRotate, Normalize,\n",
        "                            Resize, Compose, GaussNoise)\n",
        "from albumentations.pytorch import ToTensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8esyZSBIU99M",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "2fa437ff-e09f-4c29-c814-99453fe8560b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# # PARAMETERS\n",
        "\n",
        "# normalization = True\n",
        "IMG_WIDTH = 128\n",
        "IMG_HEIGHT = 128\n",
        "\n",
        "batch_size = 32\n",
        "learning_rate = 0.001\n",
        "\n",
        "numclasses = 2\n",
        "mult_chan = 1\n",
        "epochs = 750\n",
        "depth = 6\n",
        "betas = (0.5, 0.999)  # taken from paper\n",
        "contrast = 0.1\n",
        "brightness = 0.1\n",
        "saturation = 0.1\n",
        "\n",
        "optimizer_fx = 'Adam'     \n",
        "loss_fx = 'BCEWithLogitsLoss'\n",
        "\n",
        "# path to folder that houses subfolders (folder for signal and folder for ground truth)\n",
        "TRAIN_PATH = '/content/drive/My Drive/DeepLearningProject/Training Data/Mito/'\n",
        "# signal subfolder\n",
        "TRAIN_IMAGES_PATH = TRAIN_PATH + 'MitoSignal/'\n",
        "# ground-truth subfolder\n",
        "TRAIN_MASKS_PATH = TRAIN_PATH + 'MitoMask/'\n",
        "\n",
        "# path to save model parameters after optimization\n",
        "MODEL_SAVE_PATH = '/content/drive/My Drive/DeepLearningProject/'\n",
        "# filename \n",
        "MODEL_NAME = ('modelmito_' + str(IMG_WIDTH) + '_'+ str(batch_size) + '_' +\n",
        " str(learning_rate) + '_' + str(epochs) + '_' + str(depth) + '_' + str(contrast)[1:3] + \n",
        " str(brightness)[1:4] + str(saturation)[1:4] + '_' + loss_fx + '_' + optimizer_fx +'.pt')\n",
        "# print filename\n",
        "print(\"MODEL_NAME: {}\".format(MODEL_NAME))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MODEL_NAME: modelmito_128_32_0.001_750_6_.1.1.1_BCEWithLogitsLoss_Adam.pt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MFSxVIWAfem",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title read images\n",
        "\n",
        "# GRAB PATHS TO TRAINING SET IMAGES\n",
        "train_ids = next(os.walk(TRAIN_IMAGES_PATH))[2]\n",
        "train_images = np.zeros((len(train_ids), IMG_WIDTH, IMG_HEIGHT,1), dtype=np.float32)\n",
        "train_masks = np.zeros((len(train_ids), IMG_WIDTH, IMG_HEIGHT,1), dtype=np.bool)\n",
        "\n",
        "# READ IN SIGNAL FOR TRAINING\n",
        "n = 0\n",
        "num_val_chosen = 0 \n",
        "for filename in sorted(os.listdir(TRAIN_IMAGES_PATH)):\n",
        "  img = imread(os.path.join(TRAIN_IMAGES_PATH, filename))\n",
        "  img = resize(img, (IMG_WIDTH, IMG_HEIGHT), mode='constant', preserve_range=True)\n",
        "  img -= np.mean(img)\n",
        "  img /= np.std(img)\n",
        "  img[img < 0] = 0\n",
        "  train_images[n,:,:,0] = img\n",
        "  n+=1\n",
        "  if np.mod(n,10) == 0:\n",
        "    clear_output()\n",
        "    print(\"Loaded {}/{} of signal images\".format(n,len(os.listdir(TRAIN_IMAGES_PATH))))\n",
        "\n",
        "# READ IN MASKS FOR TRAINING\n",
        "n = 0\n",
        "for filename in sorted(os.listdir(TRAIN_MASKS_PATH)):\n",
        "  img = imread(os.path.join(TRAIN_MASKS_PATH, filename))\n",
        "  img = resize(img, (IMG_WIDTH, IMG_HEIGHT), mode='constant', preserve_range=True)\n",
        "  train_masks[n,:,:,0] = img\n",
        "  n+=1\n",
        "  if np.mod(n,10) == 0:\n",
        "    clear_output()\n",
        "    print(\"Loaded {}/{} of mask images\".format(n,len(os.listdir(TRAIN_IMAGES_PATH))))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSZ74eNtoszc",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title define dataset class and assign images\n",
        "\n",
        "\n",
        "# DEFINE DATASET CLASS\n",
        "class FormsDataset(Dataset):\n",
        "    def __init__(self, images, masks, num_classes: int, transforms=None):\n",
        "        self.images = images\n",
        "        self.masks = masks\n",
        "        self.num_classes = num_classes\n",
        "        self.transforms = transforms\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        mask = self.masks[idx]\n",
        "        image = image.astype(np.float32)\n",
        "        mask = mask.astype(np.bool)\n",
        "\n",
        "        if self.transforms is not None:\n",
        "          image = self.transforms(image)\n",
        "\n",
        "        return image, mask\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "#@title data augmentation\n",
        "\n",
        "my_transforms = transforms.Compose([transforms.ToPILImage(),\n",
        "                                    transforms.Grayscale(num_output_channels=1),\n",
        "                                    transforms.ColorJitter(brightness=brightness,contrast=contrast,saturation=saturation),\n",
        "                                    transforms.ToTensor()])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pd8ciUDkoNhG",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title define model layers\n",
        "# define model first; based on paper\n",
        "class SubNet2Conv(torch.nn.Module):\n",
        "    def __init__(self, n_in, n_out):\n",
        "        super().__init__()\n",
        "        self.conv1 = torch.nn.Conv2d(n_in,  n_out, kernel_size=3, padding=1)\n",
        "        self.bn1 = torch.nn.BatchNorm2d(n_out)\n",
        "        self.relu1 = torch.nn.ReLU()\n",
        "        self.conv2 = torch.nn.Conv2d(n_out, n_out, kernel_size=3, padding=1)\n",
        "        self.bn2 = torch.nn.BatchNorm2d(n_out)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        return x\n",
        "\n",
        "class MyModel(torch.nn.Module):\n",
        "  def __init__(self, n_in_channels, mult_chan=2,depth=1):\n",
        "    super().__init__()\n",
        "\n",
        "    n_out_channels = n_in_channels * mult_chan\n",
        "    self.sub_2conv_more = SubNet2Conv(n_in_channels, n_out_channels)\n",
        "\n",
        "    if depth > 0:\n",
        "        self.sub_2conv_less = SubNet2Conv(2*n_out_channels, n_out_channels)\n",
        "        self.conv_down = torch.nn.Conv2d(n_out_channels, n_out_channels, 2, stride=2)\n",
        "        self.bn0 = torch.nn.BatchNorm2d(n_out_channels)\n",
        "        self.relu0 = torch.nn.ReLU()\n",
        "            \n",
        "        self.convt = torch.nn.ConvTranspose2d(2*n_out_channels, n_out_channels, kernel_size=2, stride=2)\n",
        "        self.bn1 = torch.nn.BatchNorm2d(n_out_channels)\n",
        "        self.relu1 = torch.nn.ReLU()\n",
        "        self.sub_u = MyModel(n_out_channels,mult_chan=2,depth=depth-1)\n",
        "\n",
        "        # self.relu2 = torch.nn.ReLU()\n",
        "    self.depth = depth\n",
        "\n",
        "  def forward(self, x):\n",
        "    if self.depth == 0:\n",
        "        return self.sub_2conv_more(x)\n",
        "    else:  # depth > 0\n",
        "        x_2conv_more = self.sub_2conv_more(x)\n",
        "        x_conv_down = self.conv_down(x_2conv_more)\n",
        "        x_bn0 = self.bn0(x_conv_down)\n",
        "        x_relu0 = self.relu0(x_bn0)\n",
        "        x_sub_u = self.sub_u(x_relu0)\n",
        "        x_convt = self.convt(x_sub_u)\n",
        "        x_bn1 = self.bn1(x_convt)\n",
        "        x_relu1 = self.relu1(x_bn1)\n",
        "        x_cat = torch.cat((x_2conv_more, x_relu1), 1)  \n",
        "        x_2conv_less = self.sub_2conv_less(x_cat)\n",
        "    return x_2conv_less\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_tebWfnFTQr",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title model summary and init\n",
        "\n",
        "# # CONNECT TO GPU\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# # INITIALIZE AND ADD MODEL TO GPU\n",
        "model = MyModel(n_in_channels=1,mult_chan = mult_chan,depth=depth).to(device)\n",
        "\n",
        "# # SUMMARY\n",
        "summary(model,input_size=(1,IMG_WIDTH,IMG_HEIGHT))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgZLMoKauJ_q",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title training example image\n",
        "\n",
        "ind = np.random.randint(0,len(train_images))\n",
        "fig, ax = plt.subplots(1,2,figsize=(8,4))\n",
        "ax[0].imshow(train_images[ind,:,:,0])\n",
        "ax[0].set_title('Signal')\n",
        "ax[1].imshow(train_masks[ind,:,:,0])\n",
        "ax[1].set_title('Ground truth')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8S-BXnXQr4KI",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title train model\n",
        "\n",
        "train_dataset = FormsDataset(train_images, train_masks, numclasses, transforms=my_transforms)\n",
        "train_data_loader = DataLoader(train_dataset, batch_size=batch_size,shuffle=True)\n",
        "\n",
        "\n",
        "# Use gpu for training if available else use cpu\n",
        "\n",
        "# Here is the loss and optimizer definition\n",
        "\n",
        "# criterion = torch.nn.BCELoss()\n",
        "# m = torch.nn.LogSoftmax(dim=1)\n",
        "\n",
        "if loss_fx.lower() in 'BCEWithLogitsLoss'.lower():\n",
        "  criterion = torch.nn.BCEWithLogitsLoss()\n",
        "elif loss_fx.lower() in 'BCELoss'.lower():\n",
        "  criterion = torch.nn.BCELoss()\n",
        "elif loss_fx.lower() in 'NLLLoss'.lower():\n",
        "  criterion = torch.nn.NLLLoss()\n",
        "elif loss_fx.lower() in 'MSELoss'.lower():\n",
        "  criterion = torch.nn.MSELoss()\n",
        "elif loss_fx.lower() in 'CrossEntropyLoss'.lower():\n",
        "  criterion = torch.nn.CrossEntropyLoss()\n",
        "else\n",
        "  criterion = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "if optimizer_fx.lower() in 'Adam'.lower():\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,\n",
        "                             betas = betas)\n",
        "elif optimizer_fx.lower() in 'SGD'.lower():\n",
        "  optimizer = torch.optim.SGD(model.parameters(),lr-learning_rate)\n",
        "else\n",
        "\n",
        "\n",
        "# m = torch.nn.Sigmoid()\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer=optimizer,step_size=1,gamma=0.1)\n",
        "\n",
        "# The training loop\n",
        "total_steps = len(train_data_loader)\n",
        "iter = 0\n",
        "print(f\"{epochs} epochs, {total_steps} total_steps per epoch\")\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for i, (images, masks) in enumerate(train_data_loader, 1):\n",
        "\n",
        "        images = images.to(device)\n",
        "        masks = masks.type(torch.LongTensor)\n",
        "        masks = masks.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        masks = masks.permute(0,3,1,2)\n",
        "        loss = criterion(outputs,masks.type_as(outputs))\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "    iter +=1\n",
        "    if np.mod(iter,10) == 0:\n",
        "      print (f\"Epoch [{epoch + 1}/{epochs}], Step [{i}/{total_steps}], Loss: {loss.item():4f}\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoliF7vU0BM-",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title save model after training\n",
        "# SAVE MODEL\n",
        "torch.save(model.state_dict(),MODEL_SAVE_PATH + MODEL_NAME)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2dvMqIfJ0dY",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title check predictions on training data\n",
        "\n",
        "\n",
        "#prep training data\n",
        "# DEFINE DATASET CLASS\n",
        "class ValDataset(Dataset):\n",
        "    def __init__(self, images, num_classes: int, transforms=None):\n",
        "        self.images = images\n",
        "        self.num_classes = num_classes\n",
        "        self.transforms = transforms\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        image = image.astype(np.float32)\n",
        "        image = image\n",
        "    \n",
        "        return image\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "# read validation data\n",
        "val_dataset = ValDataset(np.moveaxis(train_images,-1,1),numclasses,None)\n",
        "val_data_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "val_images = []\n",
        "for i, images in enumerate(val_data_loader, 1):\n",
        "  images = images.to(device).type(torch.FloatTensor)\n",
        "  val_images.append(images)\n",
        "\n",
        "  \n",
        "# VISUALIZE A CELL FROM THE TRAINING SET AND SEE MODEL'S PREDICTION\n",
        "cell = np.random.randint(0,len(val_images))\n",
        "\n",
        "pred = model(val_images[cell].to(device))\n",
        "\n",
        "fig, ax = plt.subplots(2,3,figsize=(12,6))\n",
        "ax[0,0].imshow(train_images[cell,:,:,0])\n",
        "ax[0,0].set_title(\"Signal\")\n",
        "ax[0,1].imshow(pred[0,0,:,:].cpu().detach())\n",
        "ax[0,1].set_title(\"Prediction\")\n",
        "ax[0,2].imshow(train_masks[cell,:,:,0])\n",
        "ax[0,2].set_title(\"Ground-truth\")\n",
        "\n",
        "ax[1,0].hist(train_images[cell,:,:,0]);\n",
        "ax[1,1].hist(pred[0,0,:,:].cpu().detach());\n",
        "ax[1,2].hist(train_masks[cell,:,:,0]);\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
